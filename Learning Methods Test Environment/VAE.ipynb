{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#adhere to https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/input_data.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import weapon_data as weapons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "weapon_data = weapons.DataSet(seed=19071991) \n",
    "\n",
    "num_samples = weapon_data.num_examples\n",
    "input_dimension = weapon_data.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get variables\n",
    "def weights(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the networks\n",
    "#### The encoder network $q_\\phi(z|x)$\n",
    "\n",
    "The decoder network takes the input image and calculates the mean $\\mu =$ `z_mu` and the log variance $\\log\\sigma^2 =$ `z_ls2` of the Gaussian, thus producing the latent variable z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_z = 2 #Dimension of the latent space\n",
    "# Input\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_dimension]) #Batchsize x Number of Pixels\n",
    "n_hidden_1 = 5\n",
    "n_hidden_2 = 6 #one more to spot errors\n",
    "\n",
    "# First hidden layer\n",
    "W_fc1 = weights([input_dimension, n_hidden_1])\n",
    "b_fc1 = bias([n_hidden_1])\n",
    "h_1   = tf.nn.softplus(tf.matmul(x, W_fc1) + b_fc1)\n",
    "\n",
    "# Second hidden layer\n",
    "W_fc2 = weights([n_hidden_1, n_hidden_2]) \n",
    "b_fc2 = bias([n_hidden_2])\n",
    "h_2   = tf.nn.softplus(tf.matmul(h_1, W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "# Parameters for the Gaussian\n",
    "z_mu = tf.add(tf.matmul(h_2, weights([n_hidden_2, n_z])), bias([n_z]))\n",
    "# A little trick:\n",
    "#  sigma is always > 0.\n",
    "#  We don't want to enforce that the network produces only positive numbers, therefore we let \n",
    "#  the network model the parameter log(\\sigma^2) $\\in [\\infty, \\infty]$\n",
    "z_ls2 = tf.add(tf.matmul(h_2, weights([n_hidden_2, n_z])), bias([n_z])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The decoder network $p_\\theta(x|z)$ a.k.a. generator network\n",
    "\n",
    "Samples from a Gaussian using the given mean and the std. The sampling is done by addding a random number ensuring that backpropagation works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 #We have to define the batch size with the current version of TensorFlow\n",
    "eps = tf.random_normal((batch_size, n_z), 0, 1, dtype=tf.float32) # Adding a random number\n",
    "z = tf.add(z_mu, tf.multiply(tf.sqrt(tf.exp(z_ls2)), eps))  # The sampled z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 5\n",
    "n_hidden_2 = 6\n",
    "\n",
    "W_fc1_g = weights([n_z, n_hidden_1])\n",
    "b_fc1_g = bias([n_hidden_1])\n",
    "h_1_g   = tf.nn.softplus(tf.matmul(z, W_fc1_g) + b_fc1_g)\n",
    "\n",
    "W_fc2_g = weights([n_hidden_1, n_hidden_2])\n",
    "b_fc2_g = bias([n_hidden_2])\n",
    "h_2_g   = tf.nn.softplus(tf.matmul(h_1_g, W_fc2_g) + b_fc2_g)\n",
    "\n",
    "#x_mu = tf.add(tf.matmul(h_2_g,  weights([n_hidden_2, input_dimension])), bias([input_dimension]))\n",
    "#x_ls2 = tf.add(tf.matmul(h_2_g,  weights([n_hidden_2, input_dimension])), bias([input_dimension]))\n",
    "x_reconstr_mean = (tf.add(tf.matmul(h_2_g,  weights([n_hidden_2, input_dimension])), bias([input_dimension])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the loss function\n",
    "\n",
    "##### The reconstruction error\n",
    "We assume that the data x, is Gaussian distributed with diagnoal covariance matrix $\\Sigma_{ij} = \\delta_{i,j} \\sigma_i^2$. The parameters of that Gaussian are determined by the encoder network. The reconstruction error for the $i-th$ example in the min-batch is given by \n",
    "$$\n",
    "    \\mathbb{E}_{q(z|x^{(i)})}\\left( \\log\\left(p(x^{(i)}|z)\\right)\\right) \n",
    "$$\n",
    "we approximate the expectation with samplinging from the distribution (eaven with $L=1$)\n",
    "$$\n",
    "    \\mathbb{E}_{q(z|x^{(i)})}\\left( \\log\\left(p(x^{(i)}|z)\\right)\\right) \\approx \n",
    "    \\frac{1}{L} \\sum_{i=1}^L \\log\\left(p(x^{(i)}|z^{(i,l)})\\right) \\approx \\log\\left(p(x^{(i)}|z^{(i,l)})\\right)\n",
    "$$\n",
    "\n",
    "For the simple $J-dimensional$ Gaussian, we obtain the following reconstruction error (neglegting a constant term)\n",
    "$$\n",
    "    -\\log\\left(p(x^{(i)}|z^{(i)})\\right) = \\sum_{j=1}^D \\frac{1}{2} \\log(\\sigma_{x_j}^2) + \\frac{(x^{(i)}_j - \\mu_{x_j})^2}{2 \\sigma_{x_j}^2}\n",
    "$$\n",
    "\n",
    "##### The regularisation term\n",
    "\n",
    "$$\n",
    "    -D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right) = \\frac{1}{2} \\sum_{j=1}^{J} \\left(1 + \\log(\\sigma_{z_j}^{(i)^2}) - \\mu_{z_j}^{(i)^2} - \\sigma_{z_j}^{(i)^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kullbackLeibler(mu, log_sigma):\n",
    "    \"\"\"(Gaussian) Kullback-Leibler divergence KL(q||p), per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor) -> tf.Tensor\n",
    "    with tf.name_scope(\"KL_divergence\"):\n",
    "        # = -0.5 * (1 + log(sigma**2) - mu**2 - sigma**2)\n",
    "        return -0.5 * tf.reduce_sum(1 + 2 * log_sigma - mu**2 -\n",
    "                                    tf.exp(2 * log_sigma), 1)\n",
    "\n",
    "def crossEntropy(obs, actual, offset=1e-7):\n",
    "    \"\"\"Binary cross-entropy, per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        # bound by clipping to avoid nan\n",
    "        obs_ = tf.clip_by_value(obs, offset, 1 - offset)\n",
    "    return -tf.reduce_sum(actual * tf.log(obs_) + (1 - actual) * tf.log(1 - obs_), 1)\n",
    "\n",
    "def l1_loss(obs, actual):\n",
    "    \"\"\"L1 loss (a.k.a. LAD), per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "    with tf.name_scope(\"l1_loss\"):\n",
    "        return tf.reduce_sum(tf.abs(obs - actual) , 1)\n",
    "\n",
    "def l2_loss(obs, actual):\n",
    "    \"\"\"L2 loss (a.k.a. Euclidean / LSE), per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "    with tf.name_scope(\"l2_loss\"):\n",
    "        return tf.reduce_sum(tf.square(obs - actual), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstr_loss = tf.reduce_sum(0.5 * x_ls2 + (tf.square(x-x_mu)/(2.0 * tf.exp(x_ls2))), 1) #varies between implementations\n",
    "#reconstr_loss = -tf.reduce_sum(x * tf.log(1e-10 + x_reconstr_mean) + (1-x) * tf.log(1e-10 + 1 - x_reconstr_mean), 1)\n",
    "#l2 reconstr_loss = tf.reduce_sum(tf.square(x - x_reconstr_mean), 1)\n",
    "reconstr_loss = l2_loss(x_reconstr_mean, x)\n",
    "#latent_loss = kullbackLeibler(z_mu, tf.exp(z_ls2))\n",
    "latent_loss = -0.5 * tf.reduce_sum(1 + z_ls2 - tf.square(z_mu) - tf.exp(z_ls2), 1)\n",
    "cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
    "\n",
    "# Use ADAM optimizer\n",
    "optimizer =  tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run after starting [530420.9]\n",
      "Epoch:0001, cost=331802.500000000\n",
      "Epoch:0011, cost=108173.007812500\n",
      "Epoch:0021, cost=331982.937500000\n",
      "Epoch:0031, cost=202402.531250000\n",
      "Epoch:0041, cost=23465.378906250\n",
      "Epoch:0051, cost=37647.515625000\n",
      "Epoch:0061, cost=10498.804687500\n",
      "Epoch:0071, cost=63332.656250000\n",
      "Epoch:0081, cost=21577.066406250\n",
      "Epoch:0091, cost=15281.964843750\n",
      "Epoch:0101, cost=44165.578125000\n",
      "Epoch:0111, cost=8360.294921875\n",
      "Epoch:0121, cost=10160.234375000\n",
      "Epoch:0131, cost=15307.389648438\n",
      "Epoch:0141, cost=4382.636718750\n",
      "Epoch:0151, cost=14944.000000000\n",
      "Epoch:0161, cost=97967.117187500\n",
      "Epoch:0171, cost=32260.865234375\n",
      "Epoch:0181, cost=31626.935546875\n",
      "Epoch:0191, cost=3921.431884766\n",
      "Epoch:0201, cost=28113.908203125\n",
      "Epoch:0211, cost=59486.656250000\n",
      "Epoch:0221, cost=40041.589843750\n",
      "Epoch:0231, cost=101753.984375000\n",
      "Epoch:0241, cost=4276.640625000\n",
      "Epoch:0251, cost=55199.187500000\n",
      "Epoch:0261, cost=7904.743164062\n",
      "Epoch:0271, cost=18384.005859375\n",
      "Epoch:0281, cost=149949.187500000\n",
      "Epoch:0291, cost=11748.842773438\n",
      "Epoch:0301, cost=31038.490234375\n",
      "Epoch:0311, cost=14130.896484375\n",
      "Epoch:0321, cost=7479.469726562\n",
      "Epoch:0331, cost=16513.042968750\n",
      "Epoch:0341, cost=10312.235351562\n",
      "Epoch:0351, cost=37114.335937500\n",
      "Epoch:0361, cost=40261.843750000\n",
      "Epoch:0371, cost=66097.453125000\n",
      "Epoch:0381, cost=12331.029296875\n",
      "Epoch:0391, cost=17725.343750000\n",
      "Epoch:0401, cost=49479.695312500\n",
      "Epoch:0411, cost=6661.791015625\n",
      "Epoch:0421, cost=5101.605957031\n",
      "Epoch:0431, cost=4079.120117188\n",
      "Epoch:0441, cost=49691.371093750\n",
      "Epoch:0451, cost=9575.909179688\n",
      "Epoch:0461, cost=8693.664062500\n",
      "Epoch:0471, cost=42850.902343750\n",
      "Epoch:0481, cost=5623.102050781\n",
      "Epoch:0491, cost=9455.966796875\n",
      "Epoch:0501, cost=8083.214843750\n",
      "Epoch:0511, cost=2585.347167969\n",
      "Epoch:0521, cost=10083.818359375\n",
      "Epoch:0531, cost=94323.554687500\n",
      "Epoch:0541, cost=29891.013671875\n",
      "Epoch:0551, cost=30224.388671875\n",
      "Epoch:0561, cost=2734.945800781\n",
      "Epoch:0571, cost=26785.101562500\n",
      "Epoch:0581, cost=57678.375000000\n",
      "Epoch:0591, cost=38081.195312500\n",
      "Epoch:0601, cost=99953.773437500\n",
      "Epoch:0611, cost=3714.927001953\n",
      "Epoch:0621, cost=54501.453125000\n",
      "Epoch:0631, cost=7362.462890625\n",
      "Epoch:0641, cost=17805.304687500\n",
      "Epoch:0651, cost=150266.453125000\n",
      "Epoch:0661, cost=11237.112304688\n",
      "Epoch:0671, cost=30083.500000000\n",
      "Epoch:0681, cost=13475.099609375\n",
      "Epoch:0691, cost=6728.851074219\n",
      "Epoch:0701, cost=16482.718750000\n",
      "Epoch:0711, cost=10051.485351562\n",
      "Epoch:0721, cost=37008.730468750\n",
      "Epoch:0731, cost=39215.605468750\n"
     ]
    }
   ],
   "source": [
    "# This takes quite some time to converge. I am courious what would happen \n",
    "# if a proper optimizer is finally implemented in TensorFlow\n",
    "\n",
    "runs = int(148/batch_size)*10#2000 #Set to 0, for no training\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "while False:\n",
    "    sess = tf.Session()  \n",
    "    sess.run(init)\n",
    "    batch_xs = next_batch(batch_size)\n",
    "    dd = sess.run([cost], feed_dict={x: batch_xs})\n",
    "    print('Test run of cost operation in while loop results in {}'.format(dd))\n",
    "    if not np.isnan(dd) and not np.isinf(dd):\n",
    "        break\n",
    "    else:\n",
    "        sess.close()\n",
    "        \n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "batch_xs = next_batch(batch_size)\n",
    "dd = sess.run([cost], feed_dict={x: batch_xs})\n",
    "print('Test run after starting {}'.format(dd))\n",
    "\n",
    "for epoch in range(runs):\n",
    "    avg_cost = 0.\n",
    "    batch_xs = next_batch(batch_size)\n",
    "    _,d, z_mean_val, z_log_sigma_sq_val = sess.run((optimizer, cost, z_mu, z_ls2), feed_dict={x: batch_xs})\n",
    "    avg_cost += d / batch_size\n",
    "\n",
    "    # Display logs per epoch step\n",
    "    if epoch % 10 == 0:\n",
    "        save_path = saver.save(sess, \"weapon_data_model/vae.ckpt\") #Saves the weights (not the graph)\n",
    "        #print(\"Model saved in file: {}\".format(save_path))\n",
    "        print(\"Epoch:\"+ '%04d' % (epoch+1) + \", cost=\" + \"{:.9f}\".format(avg_cost))\n",
    "        #print (\"{} {} mean sigma2 {}\".format(z_mean_val.min(), z_mean_val.max(), np.mean(np.exp(z_log_sigma_sq_val))))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from weapon_data_model/vae.ckpt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument array([[ 6.7409782e+01,  2.8279922e+01,  5.9980602e+00,  1.0671598e+02,\n        -1.9471575e+00,  1.0835794e+00,  1.5019717e+00,  3.2939026e+00,\n         1.4040254e+01,  3.4450936e-01,  1.6532490e+00, -8.0291986e-01,\n         6.1896240e+02,  2.7491537e+01,  1.0997212e+01,  4.1996454e+02,\n         7.6064050e-01, -1.4602741e+00,  2.3844433e-01,  3.1823854e+00,\n         2.9108608e+00,  2.2712715e+00,  1.6112109e+00, -4.3679368e-01],\n       [ 4.7143547e+01,  1.9816837e+01,  4.2732105e+00,  7.4646439e+01,\n        -1.3100367e+00,  8.5092115e-01,  1.0970207e+00,  2.3159807e+00,\n         9.8729258e+00,  2.6554835e-01,  1.1988659e+00, -5.3215116e-01,\n         4.3251254e+02,  1.9304661e+01,  7.6553459e+00,  2.9352896e+02,\n         5.7675612e-01, -1.0075586e+00,  2.2959353e-01,  2.2375538e+00,\n         2.0603664e+00,  1.5944074e+00,  1.1246222e+00, -2.4407931e-01]],\n      dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 282\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    283\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3589\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3678\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[1;32m-> 3679\u001b[1;33m                                                            types_str))\n\u001b[0m\u001b[0;32m   3680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a ndarray into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b0984de82cf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_reconstr_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_ls2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_sample\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mx_reconstr_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_mu_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz_ls2_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstr_loss_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlatent_loss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1120\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \"\"\"\n\u001b[0;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \"\"\"\n\u001b[0;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[1;32mD:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    284\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[0;32m    285\u001b[0m                         \u001b[1;34m'must be a string or Tensor. (%s)'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                         (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[0;32m    287\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument array([[ 6.7409782e+01,  2.8279922e+01,  5.9980602e+00,  1.0671598e+02,\n        -1.9471575e+00,  1.0835794e+00,  1.5019717e+00,  3.2939026e+00,\n         1.4040254e+01,  3.4450936e-01,  1.6532490e+00, -8.0291986e-01,\n         6.1896240e+02,  2.7491537e+01,  1.0997212e+01,  4.1996454e+02,\n         7.6064050e-01, -1.4602741e+00,  2.3844433e-01,  3.1823854e+00,\n         2.9108608e+00,  2.2712715e+00,  1.6112109e+00, -4.3679368e-01],\n       [ 4.7143547e+01,  1.9816837e+01,  4.2732105e+00,  7.4646439e+01,\n        -1.3100367e+00,  8.5092115e-01,  1.0970207e+00,  2.3159807e+00,\n         9.8729258e+00,  2.6554835e-01,  1.1988659e+00, -5.3215116e-01,\n         4.3251254e+02,  1.9304661e+01,  7.6553459e+00,  2.9352896e+02,\n         5.7675612e-01, -1.0075586e+00,  2.2959353e-01,  2.2375538e+00,\n         2.0603664e+00,  1.5944074e+00,  1.1246222e+00, -2.4407931e-01]],\n      dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"weapon_data_model/vae.ckpt\")\n",
    "    x_sample = next_batch(batch_size)\n",
    "    x_reconstr_mean\n",
    "    \n",
    "    var = (x_reconstr_mean, z, z_mu, z_ls2, cost, reconstr_loss, latent_loss)\n",
    "    out = sess.run(var, feed_dict={x: x_sample})\n",
    "    x_reconstr_mean, z_vals, z_mu_val,z_ls2_val, cost_val, reconstr_loss_val,latent_loss_val = out\n",
    "    \n",
    "    print(x_reconstr_mean)\n",
    "    \n",
    "    #var = (x_mu, x_ls2, z, z_mu, z_ls2, cost, reconstr_loss, latent_loss)\n",
    "    #out = sess.run(var, feed_dict={x: x_sample})\n",
    "    #x_mu_val, x_ls2_val, z_vals, z_mu_val,z_ls2_val, cost_val, reconstr_loss_val,latent_loss_val = out\n",
    "    \n",
    "    #print(x_mu_val, x_ls2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
