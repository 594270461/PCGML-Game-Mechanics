{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#adhere to https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/input_data.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import weapon_data as weapons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weapon_data = weapons.DataSet(seed=19071991) \n",
    "\n",
    "num_samples = weapon_data.num_examples\n",
    "input_dimension = weapon_data.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get variables\n",
    "def weights(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the networks\n",
    "#### The encoder network $q_\\phi(z|x)$\n",
    "\n",
    "The decoder network takes the input image and calculates the mean $\\mu =$ `z_mu` and the log variance $\\log\\sigma^2 =$ `z_ls2` of the Gaussian, thus producing the latent variable z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_z = 3 #Dimension of the latent space\n",
    "# Input\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_dimension]) #Batchsize x Number of Pixels\n",
    "n_hidden_1 = 14\n",
    "n_hidden_2 = 15 #one more to spot errors\n",
    "\n",
    "# First hidden layer\n",
    "W_fc1 = weights([input_dimension, n_hidden_1])\n",
    "b_fc1 = bias([n_hidden_1])\n",
    "h_1   = tf.nn.softplus(tf.matmul(x, W_fc1) + b_fc1)\n",
    "\n",
    "# Second hidden layer\n",
    "W_fc2 = weights([n_hidden_1, n_hidden_2]) \n",
    "b_fc2 = bias([n_hidden_2])\n",
    "h_2   = tf.nn.softplus(tf.matmul(h_1, W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "# Parameters for the Gaussian\n",
    "z_mu = tf.add(tf.matmul(h_2, weights([n_hidden_2, n_z])), bias([n_z]))\n",
    "# A little trick:\n",
    "#  sigma is always > 0.\n",
    "#  We don't want to enforce that the network produces only positive numbers, therefore we let \n",
    "#  the network model the parameter log(\\sigma^2) $\\in [\\infty, \\infty]$\n",
    "z_ls2 = tf.add(tf.matmul(h_2, weights([n_hidden_2, n_z])), bias([n_z])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The decoder network $p_\\theta(x|z)$ a.k.a. generator network\n",
    "\n",
    "Samples from a Gaussian using the given mean and the std. The sampling is done by addding a random number ensuring that backpropagation works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10 #We have to define the batch size with the current version of TensorFlow\n",
    "eps = tf.random_normal((batch_size, n_z), 0, 1, dtype=tf.float32) # Adding a random number\n",
    "z = tf.add(z_mu, tf.multiply(tf.sqrt(tf.exp(z_ls2)), eps))  # The sampled z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1_g = weights([n_z, n_hidden_1])\n",
    "b_fc1_g = bias([n_hidden_1])\n",
    "h_1_g   = tf.nn.softplus(tf.matmul(z, W_fc1_g) + b_fc1_g)\n",
    "\n",
    "W_fc2_g = weights([n_hidden_1, n_hidden_2])\n",
    "b_fc2_g = bias([n_hidden_2])\n",
    "h_2_g   = tf.nn.softplus(tf.matmul(h_1_g, W_fc2_g) + b_fc2_g)\n",
    "\n",
    "#x_mu = tf.add(tf.matmul(h_2_g,  weights([n_hidden_2, input_dimension])), bias([input_dimension]))\n",
    "#x_ls2 = tf.add(tf.matmul(h_2_g,  weights([n_hidden_2, input_dimension])), bias([input_dimension]))\n",
    "x_reconstr_mean = (tf.add(tf.matmul(h_2_g,  weights([n_hidden_2, input_dimension])), bias([input_dimension])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the loss function\n",
    "\n",
    "##### The reconstruction error\n",
    "We assume that the data x, is Gaussian distributed with diagnoal covariance matrix $\\Sigma_{ij} = \\delta_{i,j} \\sigma_i^2$. The parameters of that Gaussian are determined by the encoder network. The reconstruction error for the $i-th$ example in the min-batch is given by \n",
    "$$\n",
    "    \\mathbb{E}_{q(z|x^{(i)})}\\left( \\log\\left(p(x^{(i)}|z)\\right)\\right) \n",
    "$$\n",
    "we approximate the expectation with samplinging from the distribution (eaven with $L=1$)\n",
    "$$\n",
    "    \\mathbb{E}_{q(z|x^{(i)})}\\left( \\log\\left(p(x^{(i)}|z)\\right)\\right) \\approx \n",
    "    \\frac{1}{L} \\sum_{i=1}^L \\log\\left(p(x^{(i)}|z^{(i,l)})\\right) \\approx \\log\\left(p(x^{(i)}|z^{(i,l)})\\right)\n",
    "$$\n",
    "\n",
    "For the simple $J-dimensional$ Gaussian, we obtain the following reconstruction error (neglegting a constant term)\n",
    "$$\n",
    "    -\\log\\left(p(x^{(i)}|z^{(i)})\\right) = \\sum_{j=1}^D \\frac{1}{2} \\log(\\sigma_{x_j}^2) + \\frac{(x^{(i)}_j - \\mu_{x_j})^2}{2 \\sigma_{x_j}^2}\n",
    "$$\n",
    "\n",
    "##### The regularisation term\n",
    "\n",
    "$$\n",
    "    -D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right) = \\frac{1}{2} \\sum_{j=1}^{J} \\left(1 + \\log(\\sigma_{z_j}^{(i)^2}) - \\mu_{z_j}^{(i)^2} - \\sigma_{z_j}^{(i)^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kullbackLeibler(mu, log_sigma):\n",
    "    \"\"\"(Gaussian) Kullback-Leibler divergence KL(q||p), per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor) -> tf.Tensor\n",
    "    with tf.name_scope(\"KL_divergence\"):\n",
    "        # = -0.5 * (1 + log(sigma**2) - mu**2 - sigma**2)\n",
    "        return -0.5 * tf.reduce_sum(1 + 2 * log_sigma - mu**2 -\n",
    "                                    tf.exp(2 * log_sigma), 1)\n",
    "\n",
    "def crossEntropy(obs, actual, offset=1e-7):\n",
    "    \"\"\"Binary cross-entropy, per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        # bound by clipping to avoid nan\n",
    "        obs_ = tf.clip_by_value(obs, offset, 1 - offset)\n",
    "    return -tf.reduce_sum(actual * tf.log(obs_) + (1 - actual) * tf.log(1 - obs_), 1)\n",
    "\n",
    "def l1_loss(obs, actual):\n",
    "    \"\"\"L1 loss (a.k.a. LAD), per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "    with tf.name_scope(\"l1_loss\"):\n",
    "        return tf.reduce_sum(tf.abs(obs - actual) , 1)\n",
    "\n",
    "def l2_loss(obs, actual):\n",
    "    \"\"\"L2 loss (a.k.a. Euclidean / LSE), per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "    with tf.name_scope(\"l2_loss\"):\n",
    "        return tf.reduce_sum(tf.square(obs - actual), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstr_loss = tf.reduce_sum(0.5 * x_ls2 + (tf.square(x-x_mu)/(2.0 * tf.exp(x_ls2))), 1) #varies between implementations\n",
    "#reconstr_loss = -tf.reduce_sum(x * tf.log(1e-10 + x_reconstr_mean) + (1-x) * tf.log(1e-10 + 1 - x_reconstr_mean), 1)\n",
    "#l2 reconstr_loss = tf.reduce_sum(tf.square(x - x_reconstr_mean), 1)\n",
    "reconstr_loss = l2_loss(x_reconstr_mean, x)\n",
    "#reconstr_loss = crossEntropy(x_reconstr_mean, x)\n",
    "#latent_loss = kullbackLeibler(z_mu, tf.exp(z_ls2))\n",
    "latent_loss = -0.5 * tf.reduce_sum(1 + z_ls2 - tf.square(z_mu) - tf.exp(z_ls2), 1)\n",
    "cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
    "\n",
    "# Use ADAM optimizer\n",
    "optimizer =  tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0000, cost=531238.496621622\n",
      "Epoch:0200, cost=52035.700485642\n",
      "Epoch:0400, cost=2453.456627098\n",
      "Epoch:0600, cost=1906.890415501\n",
      "Epoch:0800, cost=1470.416515453\n",
      "Epoch:1000, cost=761.709068917\n",
      "Epoch:1200, cost=471.787835198\n",
      "Epoch:1400, cost=399.644694973\n",
      "Epoch:1600, cost=356.897369591\n",
      "Epoch:1800, cost=324.744288470\n",
      "Epoch:2000, cost=298.241989548\n",
      "Epoch:2200, cost=292.772221952\n",
      "Epoch:2400, cost=358.749814420\n",
      "Epoch:2600, cost=307.914933901\n",
      "Epoch:2800, cost=249.197543376\n",
      "Epoch:3000, cost=227.010790851\n",
      "Epoch:3200, cost=223.909217216\n",
      "Epoch:3400, cost=226.628389616\n",
      "Epoch:3600, cost=183.061167743\n",
      "Epoch:3800, cost=204.772950250\n",
      "Epoch:4000, cost=256.768676139\n",
      "Epoch:4200, cost=196.521707483\n",
      "Epoch:4400, cost=200.910989401\n",
      "Epoch:4600, cost=174.669599275\n",
      "Epoch:4800, cost=212.098470121\n",
      "Average cost after training = 194.556886828\n"
     ]
    }
   ],
   "source": [
    "# This takes quite some time to converge. I am courious what would happen \n",
    "# if a proper optimizer is finally implemented in TensorFlow\n",
    "\n",
    "epochs = 5000 #Set to 0, for no training\n",
    "display_step = 200\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "if False:\n",
    "    while False:\n",
    "        sess = tf.Session()  \n",
    "        sess.run(init)\n",
    "        batch_xs = next_batch(batch_size)\n",
    "        dd = sess.run([cost], feed_dict={x: batch_xs})\n",
    "        print('Test run of cost operation in while loop results in {}'.format(dd))\n",
    "        if not np.isnan(dd) and not np.isinf(dd):\n",
    "            break\n",
    "        else:\n",
    "            sess.close()\n",
    "            \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(num_samples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = weapon_data.next_batch(batch_size)\n",
    "            _, d, z_mean_val, z_log_sigma_sq_val = sess.run((optimizer, cost,  z_mu, z_ls2), feed_dict={x: batch_xs})\n",
    "            avg_cost += d / num_samples * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            #save_path = saver.save(sess, \"weapon_data_model/vae.ckpt\") #Saves the weights (not the graph)\n",
    "            #print(\"Model saved in file: {}\".format(save_path))\n",
    "            print(\"Epoch:\"+ '%04d' % epoch + \", cost=\" + \"{:.9f}\".format(avg_cost))\n",
    "            #print (\"{} {} mean sigma2 {}\".format(z_mean_val.min(), z_mean_val.max(), np.mean(np.exp(z_log_sigma_sq_val))))\n",
    "    \n",
    "    print(\"Average cost after training = \" + \"{:.9f}\".format(avg_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsaver = tf.train.Saver()\\nwith tf.Session() as sess:\\n    saver.restore(sess, \"weapon_data_model/vae.ckpt\")\\n    x_sample = next_batch(batch_size)\\n    x_reconstr_mean\\n    \\n    var = (x_reconstr_mean, z, z_mu, z_ls2, cost, reconstr_loss, latent_loss)\\n    out = sess.run(var, feed_dict={x: x_sample})\\n    x_reconstr_mean, z_vals, z_mu_val,z_ls2_val, cost_val, reconstr_loss_val,latent_loss_val = out\\n    \\n    print(x_reconstr_mean)\\n    \\n    #var = (x_mu, x_ls2, z, z_mu, z_ls2, cost, reconstr_loss, latent_loss)\\n    #out = sess.run(var, feed_dict={x: x_sample})\\n    #x_mu_val, x_ls2_val, z_vals, z_mu_val,z_ls2_val, cost_val, reconstr_loss_val,latent_loss_val = out\\n    \\n    #print(x_mu_val, x_ls2_val)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"weapon_data_model/vae.ckpt\")\n",
    "    x_sample = next_batch(batch_size)\n",
    "    x_reconstr_mean\n",
    "    \n",
    "    var = (x_reconstr_mean, z, z_mu, z_ls2, cost, reconstr_loss, latent_loss)\n",
    "    out = sess.run(var, feed_dict={x: x_sample})\n",
    "    x_reconstr_mean, z_vals, z_mu_val,z_ls2_val, cost_val, reconstr_loss_val,latent_loss_val = out\n",
    "    \n",
    "    print(x_reconstr_mean)\n",
    "    \n",
    "    #var = (x_mu, x_ls2, z, z_mu, z_ls2, cost, reconstr_loss, latent_loss)\n",
    "    #out = sess.run(var, feed_dict={x: x_sample})\n",
    "    #x_mu_val, x_ls2_val, z_vals, z_mu_val,z_ls2_val, cost_val, reconstr_loss_val,latent_loss_val = out\n",
    "    \n",
    "    #print(x_mu_val, x_ls2_val)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
