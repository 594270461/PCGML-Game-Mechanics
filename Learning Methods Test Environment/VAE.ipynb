{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)\n",
    "A tutorial with code for a VAE as described in [Kingma and Welling, 2013](http://arxiv.org/abs/1312.6114). A talk with more details was given at the [DataLab Brown Bag Seminar](https://home.zhaw.ch/~dueo/bbs/files/vae.pdf).\n",
    "\n",
    "Much of the code was taken, from https://jmetzen.github.io/2015-11-27/vae.html. However, I tried to focus more on the mathematical understanding, not so much on design of the algorithm.\n",
    "\n",
    "### Some theoretical considerations \n",
    "\n",
    "#### Outline\n",
    "Situation: $x$ is from a high-dimensional space and $z$ is from a low-dimensional (latent) space, from which we like to reconstruct $p(x)$. \n",
    "\n",
    "We consider a parameterized model $p_\\theta(x|z)$ (with parameter $\\theta$), to construct x for a given value of $z$. We build this model: \n",
    "\n",
    "* $p_\\theta(x | z)$ with a neural network determening the parameters $\\mu, \\Sigma$ of a Gaussian (or as done here with a Bernoulli-Density). \n",
    "\n",
    "#### Inverting $p_\\theta(x | z)$\n",
    "\n",
    "The inversion is not possible, we therefore approximate $p(z|x)$ by $q_\\phi (z|x)$ again a combination of a NN determening the parameters of a Gaussian\n",
    "\n",
    "* $q_\\phi(z | x)$ with a neural network + Gaussian \n",
    "\n",
    "#### Training\n",
    "\n",
    "We train the network treating it as an autoencoder. \n",
    "\n",
    "#### Lower bound of the Log-Likelihood\n",
    "The likelihood cannot be determined analytically. Therefore, in a first step we derive a lower (variational) bound $L^{v}$ of the log likelihood, for a given image. Technically we assume a discrete latent space. For a continous case simply replace the sum by the appropriate integral over the respective densities. We replace the inaccessible conditional propability $p(z|x)$ with an approximation $q(z|x)$ for which we later use a neural network topped by a Gaussian.\n",
    "\n",
    "\\begin{align}\n",
    "L & = \\log\\left(p(x)\\right) &\\\\\n",
    "  & = \\sum_z q(z|x) \\; \\log\\left(p(x)\\right) &\\text{multiplied with 1 }\\\\\n",
    "  & = \\sum_z q(z|x) \\; \\log\\left(\\frac{p(z,x)}{p(z|x)}\\right) &\\\\\n",
    "  & = \\sum_z q(z|x) \\; \\log\\left(\\frac{p(z,x)}{q(z|x)} \\frac{q(z|x)}{p(z|x)}\\right) &\\\\\n",
    "  & = \\sum_z q(z|x) \\; \\log\\left(\\frac{p(z,x)}{q(z|x)}\\right) + \\sum_z q(z|x) \\; \\log\\left(\\frac{q(z|x)}{p(z|x)}\\right) &\\\\\n",
    "  & = L^{\\tt{v}} + D_{\\tt{KL}} \\left( q(z|x) || p(z|x) \\right) &\\\\\n",
    "  & \\ge L^{\\tt{v}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "The KL-Divergence $D_{\\tt{KL}}$ is always positive, and the smaller the better $q(z|x)$ approximates $p(z|x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewritting  $L^\\tt{v}$\n",
    "We split $L^\\tt{v}$ into two parts.\n",
    "\n",
    "\\begin{align}\n",
    "L^{\\tt{v}} & = \\sum_z q(z|x) \\; \\log\\left(\\frac{p(z,x)}{q(z|x)}\\right)  & \\text{with} \\;\\;p(z,x) = p(x|z) \\,p(z)\\\\\n",
    "  & =  \\sum_z q(z|x) \\; \\log\\left(\\frac{p(x|z) p(z)}{q(z|x)}\\right)  &\\\\\n",
    "  & =  \\sum_z q(z|x) \\; \\log\\left(\\frac{p(z)}{q(z|x)}\\right)  + \\sum_z q(z|x) \\; \\log\\left(p(x|z)\\right) &\\\\\n",
    "  & =  -D_{\\tt{KL}} \\left( q(z|x) || p(z) \\right)  +  \\mathbb{E}_{q(z|x)}\\left( \\log\\left(p(x|z)\\right)\\right) &\\text{putting in } x^{(i)} \\text{ for } x\\\\\n",
    "  & =  -D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right)  +  \\mathbb{E}_{q(z|x^{(i)})}\\left( \\log\\left(p(x^{(i)}|z)\\right)\\right) &\\\\\n",
    "\\end{align}\n",
    "\n",
    "Approximating $\\mathbb{E}_{q(z|x^{(i)})}$ with sampling form the distribution $q(z|x^{(i)})$\n",
    "\n",
    "#### Sampling \n",
    "With $z^{(i,l)}$ $l = 1,2,\\ldots L$ sampled from $z^{(i,l)} \\thicksim q(z|x^{(i)})$\n",
    "\\begin{align}\n",
    "L^{\\tt{v}} & = -D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right)  \n",
    "+  \\mathbb{E}_{q(z|x^{(i)})}\\left( \\log\\left(p(x^{(i)}|z)\\right)\\right) &\\\\\n",
    "L^{\\tt{v}} & \\approx -D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right)  \n",
    "+  \\frac{1}{L} \\sum_{i=1}^L \\log\\left(p(x^{(i)}|z^{(i,l)})\\right) &\\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Calculation of $D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right)$\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow version 1.8.0\n"
     ]
    }
   ],
   "source": [
    "#adhere to https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/input_data.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import weapon_data as weapons\n",
    "\n",
    "print(\"Tensor Flow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weapon_data = weapons.DataSet(seed=19071991) \n",
    "\n",
    "num_samples = weapon_data.num_examples\n",
    "\n",
    "#training params\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5000\n",
    "batch_size = 10\n",
    "epoch_debug_step = 200\n",
    "\n",
    "#network params\n",
    "input_dim = weapon_data.num_features\n",
    "hidden_1_dim = 14\n",
    "hidden_2_dim = 15 #one more to spot errors\n",
    "z_dim = 3 #Dimension of the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get variables\n",
    "def create_weight(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "    #initial = tf.contrib.layers.xavier_initializer()\n",
    "    #return tf.Variable(initial(shape))\n",
    "\n",
    "def create_bias(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "    #initial = tf.contrib.layers.xavier_initializer()\n",
    "    #return tf.Variable(initial(shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the networks\n",
    "#### The encoder network $q_\\phi(z|x)$\n",
    "\n",
    "The decoder network takes the input image and calculates the mean $\\mu =$ `z_mu` and the log variance $\\log(\\sigma^2) =$ `z_ls2` of the Gaussian, thus producing the latent variable z.\n",
    "\n",
    "##### Why $\\log(\\sigma^2)$ instead of the variance $\\sigma^2$ ?\n",
    "The variance $\\sigma^2$ is always > 0 and we don't want to enforce that the network only produces positive numbers. Therefore, we let the network model the parameter $\\log(\\sigma^2) \\in [\\infty, \\infty]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "weight = {\n",
    "    'encoder_h1': create_weight([input_dim, hidden_1_dim]),\n",
    "    'encoder_h2': create_weight([hidden_1_dim, hidden_2_dim]),\n",
    "    'z_mean': create_weight([hidden_2_dim, z_dim]),\n",
    "    'z_ls2': create_weight([hidden_2_dim, z_dim]),\n",
    "    'decoder_h1': create_weight([z_dim, hidden_1_dim]),\n",
    "    'decoder_h2': create_weight([hidden_1_dim, hidden_2_dim]),\n",
    "    'decoder_out': create_weight([hidden_2_dim, input_dim])\n",
    "}\n",
    "bias = {\n",
    "    'encoder_h1': create_bias([hidden_1_dim]),\n",
    "    'encoder_h2': create_bias([hidden_2_dim]),\n",
    "    'z_mean': create_bias([z_dim]),\n",
    "    'z_ls2': create_bias([z_dim]),\n",
    "    'decoder_h1': create_bias([hidden_1_dim]),\n",
    "    'decoder_h2': create_bias([hidden_2_dim]),\n",
    "    'decoder_out': create_bias([input_dim])\n",
    "}\n",
    "\n",
    "# Input\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_dim]) \n",
    "\n",
    "# First hidden layer\n",
    "encoder_h1 = tf.matmul(x, weight['encoder_h1']) + bias['encoder_h1']\n",
    "encoder_h1 = tf.nn.softplus(encoder_h1)\n",
    "\n",
    "# Second hidden layer\n",
    "encoder_h2 = tf.matmul(encoder_h1, weight['encoder_h2']) + bias['encoder_h2']\n",
    "encoder_h2 = tf.nn.softplus(encoder_h2)\n",
    "\n",
    "# Parameters for the Gaussian\n",
    "z_mu = tf.matmul(encoder_h2, weight['z_mean']) + bias['z_mean']\n",
    "z_ls2 = tf.matmul(encoder_h2, weight['z_ls2']) + bias['z_ls2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The decoder network $p_\\theta(x|z)$ a.k.a. generator network\n",
    "\n",
    "Samples from a Gaussian using the given mean and the std. The sampling is done by addding a random number ensuring that backpropagation works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a random number from the normal (gaussian) distribution\n",
    "eps = tf.random_normal((batch_size, z_dim), 0, 1, dtype=tf.float32) \n",
    "\n",
    "# sample z from a normal (gaussian) distribution\n",
    "z = z_mu + tf.multiply(tf.sqrt(tf.exp(z_ls2)), eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_h1 = tf.matmul(z, weight['decoder_h1']) + bias['decoder_h1']\n",
    "decoder_h1 = tf.nn.softplus(decoder_h1)\n",
    "\n",
    "decoder_h2 = tf.matmul(decoder_h1, weight['decoder_h2']) + bias['decoder_h2']\n",
    "decoder_h2 = tf.nn.softplus(decoder_h2)\n",
    "\n",
    "x_reconstr = tf.matmul(decoder_h2,  weight['decoder_out']) + bias['decoder_out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the loss function\n",
    "\n",
    "##### The reconstruction error\n",
    "We assume that the data x, is Gaussian distributed with diagnoal covariance matrix $\\Sigma_{ij} = \\delta_{i,j} \\sigma_i^2$. The parameters of that Gaussian are determined by the encoder network. The reconstruction error for the $i-th$ example in the min-batch is given by \n",
    "$$\n",
    "    \\mathbb{E}_{q(z|x^{(i)})}\\left( \\log\\left(p(x^{(i)}|z)\\right)\\right) \n",
    "$$\n",
    "we approximate the expectation with samplinging from the distribution (eaven with $L=1$)\n",
    "$$\n",
    "    \\mathbb{E}_{q(z|x^{(i)})}\\left( \\log\\left(p(x^{(i)}|z)\\right)\\right) \\approx \n",
    "    \\frac{1}{L} \\sum_{i=1}^L \\log\\left(p(x^{(i)}|z^{(i,l)})\\right) \\approx \\log\\left(p(x^{(i)}|z^{(i,l)})\\right)\n",
    "$$\n",
    "\n",
    "For the simple $J-dimensional$ Gaussian, we obtain the following reconstruction error (neglegting a constant term)\n",
    "$$\n",
    "    -\\log\\left(p(x^{(i)}|z^{(i)})\\right) = \\sum_{j=1}^D \\frac{1}{2} \\log(\\sigma_{x_j}^2) + \\frac{(x^{(i)}_j - \\mu_{x_j})^2}{2 \\sigma_{x_j}^2}\n",
    "$$\n",
    "\n",
    "##### The regularisation term\n",
    "\n",
    "$$\n",
    "    -D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right) = \\frac{1}{2} \\sum_{j=1}^{J} \\left(1 + \\log(\\sigma_{z_j}^{(i)^2}) - \\mu_{z_j}^{(i)^2} - \\sigma_{z_j}^{(i)^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kullbackLeibler(mu, log_sigma):\n",
    "    \"\"\"(Gaussian) Kullback-Leibler divergence KL(q||p), per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor) -> tf.Tensor\n",
    "    with tf.name_scope(\"KL_divergence\"):\n",
    "        # = -0.5 * (1 + log(sigma**2) - mu**2 - sigma**2)\n",
    "        return -0.5 * tf.reduce_sum(1 + 2 * log_sigma - mu**2 -\n",
    "                                    tf.exp(2 * log_sigma), 1)\n",
    "\n",
    "def crossEntropy(obs, actual, offset=1e-7):\n",
    "    \"\"\"Binary cross-entropy, per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        # bound by clipping to avoid nan\n",
    "        obs_ = tf.clip_by_value(obs, offset, 1 - offset)\n",
    "    return -tf.reduce_sum(actual * tf.log(obs_) + (1 - actual) * tf.log(1 - obs_), 1)\n",
    "\n",
    "def l1_loss(obs, actual):\n",
    "    \"\"\"L1 loss (a.k.a. LAD), per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "    with tf.name_scope(\"l1_loss\"):\n",
    "        return tf.reduce_sum(tf.abs(obs - actual) , 1)\n",
    "\n",
    "def l2_loss(obs, actual):\n",
    "    \"\"\"L2 loss (a.k.a. Euclidean / LSE), per training example\"\"\"\n",
    "    # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "    with tf.name_scope(\"l2_loss\"):\n",
    "        return tf.reduce_sum(tf.square(obs - actual), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstr_loss = tf.reduce_sum(0.5 * x_ls2 + (tf.square(x-x_mu)/(2.0 * tf.exp(x_ls2))), 1) #varies between implementations\n",
    "#reconstr_loss = -tf.reduce_sum(x * tf.log(1e-10 + x_reconstr) + (1-x) * tf.log(1e-10 + 1 - x_reconstr_mean), 1)\n",
    "#l2 reconstr_loss = tf.reduce_sum(tf.square(x - x_reconstr), 1)\n",
    "reconstr_loss = l2_loss(x_reconstr, x)\n",
    "#reconstr_loss = crossEntropy(x_reconstr, x)\n",
    "#latent_loss = kullbackLeibler(z_mu, tf.exp(z_ls2))\n",
    "latent_loss = -0.5 * tf.reduce_sum(1 + z_ls2 - tf.square(z_mu) - tf.exp(z_ls2), 1)\n",
    "cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
    "\n",
    "# Use ADAM optimizer\n",
    "optimizer =  tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if False:\n",
    "    while False:\n",
    "        sess = tf.Session()  \n",
    "        sess.run(init)\n",
    "        batch_xs = next_batch(batch_size)\n",
    "        dd = sess.run([cost], feed_dict={x: batch_xs})\n",
    "        print('Test run of cost operation in while loop results in {}'.format(dd))\n",
    "        if not np.isnan(dd) and not np.isinf(dd):\n",
    "            break\n",
    "        else:\n",
    "            sess.close()\n",
    "            \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(num_samples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = weapon_data.next_batch(batch_size)\n",
    "            _, d, z_mean_val, z_log_sigma_sq_val = sess.run((optimizer, cost,  z_mu, z_ls2), feed_dict={x: batch_xs})\n",
    "            avg_cost += d / num_samples * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % epoch_debug_step == 0:\n",
    "            save_path = saver.save(sess, \"weapon_data_model/vae.ckpt\") #Saves the weights (not the graph)\n",
    "            #print(\"Model saved in file: {}\".format(save_path))\n",
    "            print(\"Epoch:\"+ '%04d' % epoch + \", cost=\" + \"{:.9f}\".format(avg_cost))\n",
    "            #print (\"{} {} mean sigma2 {}\".format(z_mean_val.min(), z_mean_val.max(), np.mean(np.exp(z_log_sigma_sq_val))))\n",
    "    \n",
    "    print(\"Average cost after training = \" + \"{:.9f}\".format(avg_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"weapon_data_model/vae.ckpt\")\n",
    "    x_sample = next_batch(batch_size)\n",
    "    x_reconstr\n",
    "    \n",
    "    var = (x_reconstr, z, z_mu, z_ls2, cost, reconstr_loss, latent_loss)\n",
    "    out = sess.run(var, feed_dict={x: x_sample})\n",
    "    x_reconstr, z_vals, z_mu_val,z_ls2_val, cost_val, reconstr_loss_val,latent_loss_val = out\n",
    "    \n",
    "    print(x_reconstr)\n",
    "    \n",
    "    #var = (x_mu, x_ls2, z, z_mu, z_ls2, cost, reconstr_loss, latent_loss)\n",
    "    #out = sess.run(var, feed_dict={x: x_sample})\n",
    "    #x_mu_val, x_ls2_val, z_vals, z_mu_val,z_ls2_val, cost_val, reconstr_loss_val,latent_loss_val = out\n",
    "    \n",
    "    #print(x_mu_val, x_ls2_val)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "saver = tf.train.Saver()\n",
    "all_z = np.zeros((1,2))\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"weapon_data_model/vae.ckpt\")\n",
    "    total_batch = int(num_samples / batch_size) \n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = weapon_data.next_batch(batch_size)\n",
    "        x_reconstruct,z_vals,z_mean_val,z_log_sigma_sq_val  = \\\n",
    "        sess.run((x_reconstr,z, z_mu, z_ls2), feed_dict={x: batch_xs})\n",
    "        all_z = np.vstack((all_z, z_mean_val))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(all_z[:,0], all_z[:,1])\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist2d(all_z[:,0], all_z[:,1], (50, 50), cmap=plt.cm.jet)\n",
    "plt.colorbar()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
