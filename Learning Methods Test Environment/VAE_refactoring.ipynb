{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\z_outsourced_programs\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow version 1.8.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Tensor Flow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on https://jmetzen.github.io/2015-11-27/vae.html\n",
    "\n",
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
    "    \n",
    "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
    "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
    "    end-to-end.\n",
    "    \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \n",
    "    @param: session TensorFlow session to execute the network. Provided with tf.Session() or tf.InteractiveSession\n",
    "    @param: network_architecture The network architecture for this VAE. The second hidden layer is optional!\n",
    "        E.g., network_architecture = dict(n_hidden_1=20, # 1st layer encoder/decoder neurons\n",
    "                                         n_hidden_2=20, # OPTIONAL: 2nd layer encoder/decoder neurons\n",
    "                                         n_input=50, # data input\n",
    "                                         n_z=10)  # dimensionality of latent space\n",
    "    @param: optimizer Optimizer used for training the network. Provided optimizer needs to implement .minimize() function!\n",
    "        E.g., optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    @param: transfer_fct Activation function after hidden layers.\n",
    "    @param: batch_size Batch size used in training.\n",
    "    @param: print_debug Prints debug messages of this class if set to True.\n",
    "    \"\"\"\n",
    "    def __init__(self, session, network_architecture, optimizer, transfer_fct=tf.nn.tanh, batch_size=1, print_debug=True):\n",
    "        self._print_debug = print_debug\n",
    "        self.__print_debug(\"Start initializing variational autoencoder (VAE) ...\")\n",
    "        \n",
    "        self._network_architecture = network_architecture\n",
    "        self._transfer_fct=transfer_fct\n",
    "        self._optimizer_provided = optimizer\n",
    "        self._batch_size = batch_size\n",
    "        self._has_2_hidden_layer = ('n_hidden_2' in network_architecture)\n",
    "        self.__print_debug(\"Does VAE have 2 hidden layers? \" + str(self._has_2_hidden_layer), 1)\n",
    "        \n",
    "        # Input\n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, network_architecture[\"n_input\"]], name=\"input_X\") \n",
    "        \n",
    "        # create the whole variational autoencoder network \n",
    "        self.__create_network()\n",
    "        \n",
    "        self.__create_loss_optimizer()\n",
    "        \n",
    "        #launch a tensorflow session\n",
    "        self._session = session\n",
    "        self._train_saver = tf.train.Saver()\n",
    "\n",
    "        #run the initializer for all tensorflow variables\n",
    "        self._session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.__print_debug(\"VAE ready to use!\")\n",
    "\n",
    "    def train_with_mini_batch(self, batch):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        opt, cost = self._session.run((self.optimizer, self.cost), feed_dict={self.X: batch})\n",
    "        return cost\n",
    "    \n",
    "    def encode_to_latent_space(self, x):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self._session.run(self.z_mean, feed_dict={self.X: x})\n",
    "    \n",
    "    def decode_from_latent_space(self, z):\n",
    "        \"\"\" Generate data by sampling from latent space. \"\"\"\n",
    "        return self._session.run(self.x_reconstructed, feed_dict={self.z: z})\n",
    "    \n",
    "    def encode_and_decode(self, x):\n",
    "        \"\"\" Use VAE to reconstruct given data. Encodes and decodes in the same run.\"\"\"\n",
    "        return self._session.run(self.x_reconstructed, feed_dict={self.X: x})\n",
    "    \n",
    "    def load_trained_model(self, save_path):\n",
    "        ''' Loads trained model from disk. CAUTION: need to open session with 'tf.Session(graph=tf.Graph())'!'''\n",
    "        self._train_saver.restore(self._session, save_path)\n",
    "        self.__print_debug(\"Trained model found in '\"+save_path+\"' restored!\")\n",
    "        \n",
    "    def save_trained_model(self, path):\n",
    "        save_path = self._train_saver.save(self._session, path + \"model.ckpt\") #Saves the weights (not the graph)\n",
    "        self.__print_debug(\"Model saved in file: {}\".format(save_path))\n",
    "        return save_path\n",
    "    \n",
    "    def __print_debug(self, message, indent=0):\n",
    "        if self._print_debug:\n",
    "            tabs=\"\"\n",
    "            for i in range(indent):\n",
    "                tabs += \"\\t\"\n",
    "            print(tabs+message)\n",
    "\n",
    "    def __create_network(self):\n",
    "        self.__print_debug(\"Start creating VAE network ...\", 1)\n",
    "        #init weights and biases of all network nodes\n",
    "        \n",
    "        weights_and_biases = self.__init_weights_and_biases()\n",
    "        \n",
    "        #create the encoder network which generates the latent network\n",
    "        self.z_mean, self.z_log_sigma_sq = \\\n",
    "            self.__create_encoder_network(weights_and_biases['weights_encoder'], \n",
    "                                          weights_and_biases['biases_encoder'])\n",
    "            \n",
    "        #create the sampling operation to:\n",
    "        # -) map our data distribution to a normal distribution \n",
    "        # -) and secure a working backpropagation\n",
    "        self.z = self.__create_z_sampling_operation()\n",
    "        \n",
    "        #create the decoder network which generates a reconstruction of input X\n",
    "        self.x_reconstructed = \\\n",
    "            self.__create_decoder_network(weights_and_biases['weights_decoder'], \n",
    "                                          weights_and_biases['biases_decoder'])\n",
    "        \n",
    "        self.__print_debug(\"Finished creating VAE network!\", 1)\n",
    "\n",
    "        \n",
    "    def __create_encoder_network(self, weights, biases):\n",
    "        self.__print_debug(\"Start creating encoder network ...\", 2)\n",
    "\n",
    "        hidden_layer = self.__create_hidden_layer(self.X, weights, biases)\n",
    "        \n",
    "        # Parameters for the Gaussian\n",
    "        z_mean = tf.matmul(hidden_layer, weights['z_mean']) + biases['z_mean']\n",
    "        z_log_sigma_sq = tf.matmul(hidden_layer, weights['z_ls2']) + biases['z_ls2']\n",
    "        \n",
    "        self.__print_debug(\"Finished creating encoder network!\", 2)\n",
    "        \n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "        \n",
    "    def __create_z_sampling_operation(self):\n",
    "        self.__print_debug(\"Start creating z (latent layer) sampling operation ...\", 2)\n",
    "        \n",
    "        n_z = self._network_architecture['n_z']\n",
    "        \n",
    "        # Adding a random number from the normal (gaussian) distribution\n",
    "        epsilon = tf.random_normal((self._batch_size, n_z), 0, 1, dtype=tf.float32) \n",
    "\n",
    "        # sample z from a normal (gaussian) distribution -> z = mu + sigma*epsilon\n",
    "        z = self.z_mean + tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), epsilon)\n",
    "        \n",
    "        self.__print_debug(\"Finished creating z sampling operation!\", 2)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def __create_decoder_network(self, weights, biases):\n",
    "        self.__print_debug(\"Start creating decoder/generator network ...\", 2)\n",
    "        \n",
    "        hidden_layer = self.__create_hidden_layer(self.z, weights, biases)\n",
    "        \n",
    "        x_reconstructed = tf.matmul(hidden_layer,  weights['out']) + biases['out']\n",
    "    \n",
    "        self.__print_debug(\"Finished creating decoder/generator network!\", 2)\n",
    "            \n",
    "        return x_reconstructed\n",
    "    \n",
    "    def __create_hidden_layer(self, x, weights, biases):\n",
    "        self.__print_debug(\"Start creating hidden layer ...\", 3)\n",
    "        \n",
    "        # First hidden layer\n",
    "        hidden_layer_1 = tf.matmul(x, weights['h1']) + biases['h1']\n",
    "        hidden_layer_1 = self._transfer_fct(hidden_layer_1)\n",
    "\n",
    "        # Second hidden layer\n",
    "        hidden_layer_2 = tf.matmul(hidden_layer_1, weights['h2']) + biases['h2']\n",
    "        hidden_layer_2 = self._transfer_fct(hidden_layer_2)\n",
    "\n",
    "        self.__print_debug(\"Finished creating hidden layer!\", 3)\n",
    "        \n",
    "        return hidden_layer_2 if self._has_2_hidden_layer else hidden_layer_1\n",
    "        \n",
    "    def __init_weights_and_biases(self):\n",
    "        self.__print_debug(\"Start initalizing weights and biases ...\", 2)\n",
    "\n",
    "        n_input = self._network_architecture['n_input']\n",
    "        n_hidden_1 = self._network_architecture['n_hidden_1']\n",
    "        n_hidden_2 = self._network_architecture['n_hidden_2'] if self._has_2_hidden_layer else 1\n",
    "        n_hidden_out = n_hidden_2 if self._has_2_hidden_layer else n_hidden_1\n",
    "        n_z = self._network_architecture['n_z']\n",
    "        \n",
    "        weights_and_biases = dict()\n",
    "        \n",
    "        #encoder\n",
    "        weights_and_biases['weights_encoder'] = {\n",
    "            'h1': self.__create_weight([n_input, n_hidden_1], \"w_enc_h1\"),\n",
    "            'h2': self.__create_weight([n_hidden_1, n_hidden_2], \"w_enc_h2\"),\n",
    "            'z_mean': self.__create_weight([n_hidden_out, n_z], \"w_z_mean\"),\n",
    "            'z_ls2': self.__create_weight([n_hidden_out, n_z], \"w_z_ls2\")\n",
    "        }\n",
    "        weights_and_biases['biases_encoder'] = {\n",
    "            'h1': self.__create_bias([n_hidden_1], \"b_enc_h1\"),\n",
    "            'h2': self.__create_bias([n_hidden_2], \"b_enc_h2\"),\n",
    "            'z_mean': self.__create_bias([n_z], \"b_z_mean\"),\n",
    "            'z_ls2': self.__create_bias([n_z], \"b_z_ls2\")\n",
    "        }\n",
    "        \n",
    "        #decoder\n",
    "        weights_and_biases['weights_decoder'] = {\n",
    "            'h1': self.__create_weight([n_z, n_hidden_1], \"w_dec_h1\"),\n",
    "            'h2': self.__create_weight([n_hidden_1, n_hidden_2], \"w_dec_h2\"),\n",
    "            'out': self.__create_weight([n_hidden_out, n_input], \"w_out\")\n",
    "        }\n",
    "        weights_and_biases['biases_decoder'] = {\n",
    "            'h1': self.__create_bias([n_hidden_1], \"b_dec_h1\"),\n",
    "            'h2': self.__create_bias([n_hidden_2], \"b_dec_h2\"),\n",
    "            'out': self.__create_bias([n_input], \"b_out\")\n",
    "        }\n",
    "        self.__print_debug(\"Finished initalizing weights and biases!\", 2)\n",
    "\n",
    "        return weights_and_biases\n",
    "    \n",
    "    def __create_loss_optimizer(self):\n",
    "        self.__print_debug(\"Start creating optimizer/backprop operation ...\", 1)\n",
    "\n",
    "        # The loss is composed of two terms:\n",
    "        #according to the VAE tutorial paper:\n",
    "        reconstr_loss = tf.reduce_sum(tf.square(tf.abs(self.X - self.x_reconstructed)), 1)\n",
    "        #reconstr_loss = self.__l2_loss(self.x_reconstructed, self.X)\n",
    "        #reconstr_loss = self.__cross_entropy(self.x_reconstructed, self.X)\n",
    "        \n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "        ##    between the distribution in latent space induced by the encoder on \n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for transmitting the the latent space distribution given\n",
    "        #     the prior.\n",
    "        latent_loss = self.__kullback_leibler(self.z_mean, tf.log(tf.sqrt(tf.exp(self.z_log_sigma_sq))))\n",
    "        \n",
    "        self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
    "        self.optimizer = self._optimizer_provided.minimize(self.cost)\n",
    "\n",
    "        self.__print_debug(\"Finished creating optimizer/backprop operation!\", 1)\n",
    "\n",
    "            \n",
    "    def __create_weight(self, shape, name=\"\"):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, dtype=tf.float32, name=name)\n",
    "        #initial = tf.contrib.layers.xavier_initializer()\n",
    "        #return tf.Variable(initial(shape), dtype=tf.float32, name=name)\n",
    "\n",
    "    def __create_bias(self, shape, name=\"\"):\n",
    "        initial = tf.constant(0.1, shape=shape, dtype=tf.float32)\n",
    "        return tf.Variable(initial, name=name)\n",
    "        #return tf.Variable(tf.zeros(shape, dtype=tf.float32), name=name)\n",
    "        #initial = tf.contrib.layers.xavier_initializer()\n",
    "        #return tf.Variable(initial(shape), dtype=tf.float32, name=name)\n",
    "        \n",
    "    def __kullback_leibler(self, mu, log_sigma):\n",
    "        \"\"\"(Gaussian) Kullback-Leibler divergence KL(q||p), per training example\"\"\"\n",
    "        # (tf.Tensor, tf.Tensor) -> tf.Tensor\n",
    "        with tf.name_scope(\"KL_divergence\"):\n",
    "            # = -0.5 * (1 + log(sigma**2) - mu**2 - sigma**2)\n",
    "            return -0.5 * tf.reduce_sum(1 + 2 * log_sigma - mu**2 - tf.exp(2 * log_sigma), 1)\n",
    "\n",
    "    def __cross_entropy(self, obs, actual, offset=1e-7):\n",
    "        \"\"\"Binary cross-entropy, per training example\"\"\"\n",
    "        # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            # bound by clipping to avoid nan (--> log(0))\n",
    "            obs_ = tf.clip_by_value(obs, offset, 1 - offset)\n",
    "            return -tf.reduce_sum(actual * tf.log(obs_) + (1 - actual) * tf.log(1 - obs_), 1)\n",
    "\n",
    "    def __l2_loss(self, obs, actual):\n",
    "        \"\"\"L2 loss (a.k.a. Euclidean / LSE), per training example\"\"\"\n",
    "        # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "        with tf.name_scope(\"l2_loss\"):\n",
    "            return tf.reduce_sum(tf.square(obs - actual), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL_PATH = \"./trained_vae/\"\n",
    "\n",
    "def train_vae(vae, weapon_data, batch_size=1, training_epochs=10, epoch_debug_step=5,\n",
    "              trained_model_save_path=DEFAULT_MODEL_PATH, \n",
    "              save_model = True, save_model_every_epoch = True):\n",
    "    \n",
    "    num_samples = weapon_data.num_examples\n",
    "    trained_model_path = \"\"\n",
    "    \n",
    "    #training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(num_samples / batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch = weapon_data.next_batch(batch_size)\n",
    "            \n",
    "            # Fit training using batch data\n",
    "            cost = vae.train_with_mini_batch(batch)\n",
    "            \n",
    "            #compute average loss/cost\n",
    "            avg_cost += cost / num_samples * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % epoch_debug_step == 0:\n",
    "            if save_model and save_model_every_epoch:\n",
    "                trained_model_path = vae.save_trained_model(trained_model_save_path)\n",
    "            print(\"Epoch:\"+ '%04d' % (epoch+1) + \", cost=\" + \"{:.9f}\".format(avg_cost))\n",
    "            \n",
    "    if save_model and not save_model_every_epoch:\n",
    "        trained_model_path = vae.save_trained_model(trained_model_save_path)\n",
    "    \n",
    "    if save_model:\n",
    "        print(\"Trained model saved! You can find it in '\"+trained_model_path+\"'\")\n",
    "\n",
    "#just a convenience wrapper\n",
    "def get_untrained_vae(session, network_architecture, optimizer, transfer_fct, batch_size=1):\n",
    "    \n",
    "    return VariationalAutoencoder(session=session, network_architecture=network_architecture, optimizer=optimizer, \n",
    "                                 transfer_fct=transfer_fct, batch_size=batch_size, print_debug=False)\n",
    "\n",
    "#just a convenience wrapper\n",
    "def get_new_trained_vae(session, weapon_data, network_architecture, optimizer, transfer_fct,\n",
    "                          batch_size=1, training_epochs=10, epoch_debug_step=5,\n",
    "                          trained_model_save_path=DEFAULT_MODEL_PATH, \n",
    "                          save_model = True, save_model_every_epoch = True):\n",
    "\n",
    "    #create vae\n",
    "    vae = get_untrained_vae(session=session, network_architecture=network_architecture, optimizer=optimizer, \n",
    "                                 transfer_fct=transfer_fct, batch_size=batch_size)\n",
    "    \n",
    "    #train it\n",
    "    vae = train_vae(vae, weapon_data, batch_size, training_epochs, epoch_debug_step, \n",
    "                       trained_model_save_path, save_model, save_model_every_epoch)\n",
    "        \n",
    "    return vae\n",
    "\n",
    "#just a convenience wrapper\n",
    "def restore_vae(untrained_vae, model_path):\n",
    "    untrained_vae.load_trained_model(model_path)\n",
    "    return untrained_vae\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test utils\n",
    "\n",
    "def print_decoded_tensors_as_dict(weapon_data, array_of_tensors):\n",
    "    genDict = {}\n",
    "    \n",
    "    for tensor in array_of_tensors:\n",
    "        decoded = weapon_data.decode_processed_tensor(tensor)\n",
    "        \n",
    "        for key, value in decoded.items():\n",
    "            if key not in genDict:\n",
    "                genDict[key] = []\n",
    "            genDict[key].append(value)\n",
    "    \n",
    "    for key, value in genDict.items():\n",
    "            print(key, \"=\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001, cost=23.786172719\n",
      "Epoch:0002, cost=21.236963710\n",
      "Epoch:0003, cost=19.748965043\n",
      "Epoch:0004, cost=18.168292236\n",
      "Epoch:0005, cost=17.481189422\n",
      "Epoch:0006, cost=16.301139890\n",
      "Epoch:0007, cost=14.470915579\n",
      "Epoch:0008, cost=14.067361136\n",
      "Epoch:0009, cost=13.531242574\n",
      "Epoch:0010, cost=12.468845714\n",
      "Trained model saved! You can find it in './trained_vae/model.ckpt'\n"
     ]
    }
   ],
   "source": [
    "import weapon_data as weapons\n",
    "\n",
    "network_architecture = \\\n",
    "    dict(n_input=0, #set it in with scope\n",
    "         n_hidden_1=14,\n",
    "         n_hidden_2=15,\n",
    "         n_z=2)  \n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer =  tf.train.AdamOptimizer(learning_rate)\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "transfer_fct = lambda x : tf.nn.tanh(x)\n",
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "epoch_debug_step = 1\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    weapon_data = weapons.DataSet(seed=19071991) \n",
    "    network_architecture['n_input'] = weapon_data.num_features\n",
    "  \n",
    "    vae = get_new_trained_vae(sess, weapon_data, network_architecture, optimizer, \n",
    "                              transfer_fct, batch_size, num_epochs, epoch_debug_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trained_vae/model.ckpt\n",
      "[[-0.70326645 -0.48674743 -0.30162464 -0.29826301 -0.22336265  1.3003496\n",
      "  -0.75793673 -0.59816232  0.19378264 -1.11913753 -0.86609068 -0.28002455\n",
      "   0.55030494  2.19514536  3.58362519  0.55859823 -0.28577464 -0.32322997\n",
      "  -0.50421948 -0.54611868 -0.36023741 -0.52522573  2.45919073 -0.2672411 ]]\n",
      "[[-0.7708178  -0.498041   -0.42327693 -0.55981416 -0.2932658   1.1938373\n",
      "  -0.68055475 -0.5966249   1.163441   -0.5930207  -0.8460628  -0.311187\n",
      "   0.0114214   0.46242338  0.04673539  1.3603003  -0.29915944 -0.5636202\n",
      "  -0.18732625 -0.23165965  0.90525615 -0.37399954  0.6732751  -0.29865676]]\n",
      "damages_first = ['26.5', '24.827892668055533']\n",
      "damages_last = ['20.0', '19.7291307952624']\n",
      "dmg_distances_first = ['11.0', '9.191911782708633']\n",
      "dmg_distances_last = ['50.0', '39.09313911245435']\n",
      "drag = ['0.0024999999441206455', '0.002415416674981484']\n",
      "firemode_Automatic = ['1.0', '0.9485291973187807']\n",
      "firemode_Semi-Automatic = ['0.0', '0.03725108587539483']\n",
      "hiprecoildec = ['6.0', '8.645005553332595']\n",
      "hiprecoilright = ['0.15000000596046448', '0.35715616513399323']\n",
      "hiprecoilup = ['0.20000000298023224', '0.2563072732135283']\n",
      "hordispersion = ['0.0', '-0.016241562597875042']\n",
      "initialspeed = ['700.0', '587.1088946261773']\n",
      "magsize = ['120.0', '46.27112468012588']\n",
      "reloadempty = ['8.75979995727539', '3.937082352488511']\n",
      "rof = ['450.0', '627.6839984693581']\n",
      "shotspershell = ['1.0', '0.9613911923164031']\n",
      "type_Shotgun = ['-1.3877787807814457e-17', '-0.07035120433877629']\n",
      "type_Pistol = ['2.7755575615628914e-17', '0.12739514546011174']\n",
      "type_Rifle = ['2.7755575615628914e-17', '0.13228001633750808']\n",
      "type_Submachine Gun = ['0.0', '0.40351374578889326']\n",
      "type_Sniper Rifle = ['0.0', '0.06225429145083508']\n",
      "verdispersion = ['0.0', '-0.014972451596137831']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    vae = get_untrained_vae(sess, network_architecture, optimizer, transfer_fct, batch_size)\n",
    "    vae = restore_vae(vae, \"trained_vae/model.ckpt\")\n",
    "        \n",
    "    weapon_data = weapons.DataSet(seed=19071991) \n",
    "    samples = weapon_data.next_batch(batch_size)\n",
    "    x_reconstructed = vae.encode_and_decode(samples)\n",
    "    \n",
    "    print(samples)\n",
    "    print(x_reconstructed)\n",
    "    \n",
    "    print_decoded_tensors_as_dict(weapon_data, np.concatenate((samples,x_reconstructed), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
